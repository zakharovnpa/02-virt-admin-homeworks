# "5.5. Оркестрация кластером Docker контейнеров на примере Docker Swarm" - Захаров Сергей Николаевич

## Задача 1

Дайте письменые ответы на следующие вопросы:

- В чём отличие режимов работы сервисов в Docker Swarm кластере: replication и global?
- Какой алгоритм выбора лидера используется в Docker Swarm кластере? Смотрите слайд с Полезными материалами.
- Что такое Overlay Network?

**Ответы:**
#### В чём отличие режимов работы сервисов в Docker Swarm кластере: replication и global?
Есть два типа выполнения (развертывания) сервисов:
 - Replicated. Он находится в стольких экземплярах, сколько мы задаем. Для реплицированной службы мы указываем количество идентичных задач, которые хотим запустить. Например, мы решили развернуть службу HTTP с тремя репликами, каждая из которых обслуживает один и тот же контент.
 - Global. В этом режиме сервисы запускаются на всех нодах одновременно. Обычно режим Global используется для различных экспортеров, чтобы обеспечить мониторинг всего кластера. Также для всех сущностей, которые нужно чтобы они в моменте находились на каждой ноде, неважно Worker это или Manager. 

  Самый типичный пример режима выполнения сервиса Global - это экспортеры для передачи метрик на сервер мониторинга. Хорошими кандидатами на роль глобальных служб являются агенты мониторинга, антивирусные сканеры или другие типы контейнеров, которые мы хотим запускать на каждй ноде кластера.

  Глобальная служба - это служба, которая запускает одну задачу на каждй ноде. Предварительно заданного количества задач нет. Каждый раз, когда мы добавляем ноду в кластер, оркестратор создает задачу, а планировщик назначает задачу новой ноде. 
#### Какой алгоритм выбора лидера используется в Docker Swarm кластере? 
В Docker Swarm кластере для выбора лидера используется алгоритм "Raft" - алгоритм управления реплицированным журналом. 

Raft реализует консенсус, сначала выбирая выдающегося лидера, а затем возлагая на него полную ответственность за управление реплицированным журналом. Лидер принимает записи журнала от клиентов, реплицирует их на другие серверы и сообщает серверам, когда можно безопасно применять записи журнала к их конечным машинам. Наличие лидера упрощает управление  реплицированным журналом. Например, лидер может решить, где разместить новые записи в журнале, не консультируясь с другими серверами, и данные будут передаваться простым способом от лидера к другим серверам. Лидер может выйти из строя или отключиться от других серверов, и в этом случае будет избран новый лидер. 

- Учитывая подход лидера, Raft разбивает проблему консенсуса на три относительно независимых подзадачи:
  - Выборы лидера: новый лидер должен быть выбран, когда существующий лидер терпит поражение
  - Репликация журналов: лидер должен принимать записи журналов от клиентов и реплицировать их по кластеру, заставляя другие журналы согласовывать свои собственные 
журналы.
  - Безопасность: ключевым свойством безопасности для Raft является свойство безопасности State Machine: если какой-либо сервер применил конкретную 
запись журнала к своему конечному состоянию, то никакой другой сервер не может применить другую команду для того же индекса журнала.
#### Что такое Overlay Network?
**Overlay Network** в контексте  Docker Swarm - это созданная отдельным протоколом связи распределенная сеть между нодами демона кластера поверх основной сети для обеспечения взаимодействия и передачи служебной и сервисной информации. Сети оверлей - закрытые с применением шифрования с использованием алгоритма AES в режиме GCM. Узлы-менеджеры в кластере меняют ключ, используемый для шифрования передаваемых данных каждые 12 часов. Маршрутизация трафика идет непосредственно между нодами кластера.

[Подробнее](https://docs.docker.com/network/overlay/)

## Задача 2

Создать ваш первый Docker Swarm кластер в Яндекс.Облаке

Для получения зачета, вам необходимо предоставить скриншот из терминала (консоли), с выводом команды:
```
docker node ls
```

**Ответ:** 

На скриншоте представлены доступные ноды в кластере. Лидером является node02 (после перезагрузки лидера node01)
![docker-node-ls](/05-virt-05-docker-swarm/Lecture/img/docker-node-ls.png)

## Задача 3

Создать ваш первый, готовый к боевой эксплуатации кластер мониторинга, состоящий из стека микросервисов.

Для получения зачета, вам необходимо предоставить скриншот из терминала (консоли), с выводом команды:
```
docker service ls
```

**Ответ:** 

На скриншоте представлены доступные сервисы в кластере. Показано сколько реплик каждого соервиса активно и какой режим репликации включен.
![docker-service-ls](/05-virt-05-docker-swarm/Lecture/img/docker-service-ls.png)

## Задача 4 (*)

Выполнить на лидере Docker Swarm кластера команду (указанную ниже) и дать письменное описание её функционала, что она делает и зачем она нужна:
```
# см.документацию: https://docs.docker.com/engine/swarm/swarm_manager_locking/


docker swarm update --autolock=true
```
**Ответ:**

![docker-swarm-update](/05-virt-05-docker-swarm/Lecture/img/docker-swarm-update.png)
* Команда ` docker swarm update --autolock=true ` включает автоблокировку кластера.

Речь идет о функции автоблокировки, когда Docker защищает общий ключ шифрования TLS и ключ, используемый для шифрования и дешифрования журналов Raft, позволяя нам стать владельцем этих ключей и иметь возможность ручной разблокировки менеджеров кластера.

Журналы Raft, используемые менеджерами кластера, по умолчанию зашифрованы на диске. Это шифрование защищает конфигурацию и данные от злоумышленников, которые получают доступ к зашифрованным журналам Raft. Одна из причин, по которой эта функция была представлена, заключалась в поддержке функции безопасности Docker.

При перезапуске Docker ключ TLS, используемый для шифрования связи между узлами кластреа, и ключ, используемый для шифрования и дешифрования журналов Raft на диске, загружаются в память каждого узла-менеджера. 

Когда Docker перезапускается, мы должны сначала разблокировать кластер, используя ключ шифрования, сгенерированный Docker, когда кластер был заблокирован. Мы можем изменить этот ключ шифрования в любое время.

[Подробнее](https://docs.docker.com/engine/swarm/swarm_manager_locking/)
